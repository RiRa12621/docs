---
title: Introducing Prometheus Agent Mode, an Efficient Cloud Native Way for Forwarding Metrics 
created_at: 2021-11-02
kind: article
author_name: Bartlomiej Plotka (@bwplotka)
---

> Bartek P≈Çotka is a Prometheus Maintainer since 2019 and Principal Software Engineer at Red Hat. Co-author of the CNCF Thanos project. CNCF Ambassador and tech lead for the CNCF TAG Observability. In his free time he writes a book titled "Efficient Go" with O'Reilly. Opinions are my own!

What I personally love in Prometheus project and one of the many reasons why I joined the team, was the laser focus on the project goals. Prometheus was always about moving boundaries when it comes to providing pragmatic, reliable, cheap, yet invaluable metric monitoring. Its ultra stable and robust APIs, querying language and integration protocols (e.g. Remote Write and [Open Metrics](https://openmetrics.io/)) allowed the CNCF metric ecosystem to grow on those strong foundations. This is why we don't see Kubernetes clusters without Prometheus running there. This is also why we can see community exporters for getting metrics about literally everything e.g. [containers](https://github.com/google/cadvisor), [eBPF](https://github.com/cloudflare/ebpf_exporter), [Minecraft server statistics](https://github.com/sladkoff/minecraft-prometheus-exporter) and even [plants health when gardening](https://megamorf.gitlab.io/2019/07/14/monitoring-plant-health-with-prometheus/). 

The strong focus of the Prometheus community allowed other open source projects to grow (e.g. [Cortex](https://cortexmetrics.io/), [Thanos](https://thanos.io/) and more) to extend Prometheus data model beyond single clusters. Not mentioning cloud vendors adopting Prometheus API and data model (e.g. [Amazon Managed Prometheus](https://aws.amazon.com/prometheus/), [Google Cloud Managed Prometheus](https://cloud.google.com/stackdriver/docs/managed-prometheus), [Grafana Cloud](https://grafana.com/products/cloud/) and more). If you are looking for a single reason why Prometheus project is so successful, it is this: *Focusing the monitoring community on what matters*.

In this blog post I would love to introduce you a new mode of running Prometheus called "agent". It is built directly into the Prometheus binary. The agent mode blocks some of the Prometheus features and optimizes the binary for scraping and remote writing to remote location. While reducing number of features on the solution sounds like a regression, I will explain you why it is a game changer for certain deployments in the CNCF ecosystem. I am personally super excited for this! (:

## History of the Forwarding Use Case

The core design of Prometheus has been unchanged for the whole project lifetime. Inspired by the Google Borgmon, you can deploy it next to the application you want to monitor, tell Prometheus where they are and allow Prometheus to scrape the current values of their metrics every configured interval. Such collection method, which is often referred as "pull model" is the core principle that allows Prometheus to be lightweight and ultra reliable. Furthermore, it allows applications and exporters to be dead simple as they need to provide a simple human-readable HTTP page with current metric values (in Open Metric format), without complex push infrastructure as we typically see. Overall, in simple view, typical Prometheus monitoring deployment looks as presented below:

![Prometheus high level view](/assets/blog/2021-11-02/prom.png)

This works great, and we have seen millions successful deployments like this over the years that process millions of series. Some of them with longer time retention like 2 years so. All, allowing to query, alert and record metrics useful for both cluster admins, as well as developers.

However, the CNCF world constantly grows and evolves. With the grow of managed Kubernetes solutions and clusters created on demand within seconds we are now able to treat "clusters" as a "cattle" not "pet". In some cases solutions does not even have the cluster notion anymore e.g. [kcp](https://github.com/kcp-dev/kcp), [Google Cloud Run](https://cloud.google.com/run), [Fargate](https://aws.amazon.com/fargate/) and other serverless platforms. 

What that means? That means monitoring data has to be somehow aggregated and presented to users on *global* level. This is often called a *Global View* feature. First things first. You must not naively implement this by putting Prometheus on global level and scrape metrics cross networks, neither push metric directly from application to central location for monitoring purposes. Let me explain why those are bad ideas:

* Scrape across network is a terrible idea as we employ a huge amount of unknowns to our monitoring pipeline. The pull model allows Prometheus to know *why exactly* metric target has problems and when. Maybe it's down, misconfigured, restarted, too slow to give us metrics (e.g CPU saturated), not discoverable by service discovery, we don't have credentials to access or just DNS, network or whole cluster is down. By putting our scraper outside of the network we risk losing some of the information due to unreliable scrape and irregular scrape intervals. Don't do it, it's not worth it. (:
* Metric push directly from the application to some central location is equally bad. Especially when you monitor larger fleet you know literally *nothing* when you don't see metrics from remote application. Is application down? Is my receive pipeline down? Maybe application failed to authorize? Maybe it failed to get IP of my remote cluster? Maybe it's too slow? Maybe network is down? Such design is often a recipe for a failure (or at least shrug ü§∑üèΩ‚Äç‚ôÄÔ∏è moments).

As discussed many times on various conferences, Prometheus introduced three main protocols to support the global view case. Each with its own pros and cons. Let's go briefly go through those. They are presented in orange colour on the diagram below.

![Prometheus aggregations](/assets/blog/2021-11-02/prom-remote.png)

* Federation was introduced as the first feature for aggregation purposes. It allows a special scrape done by Prometheus on global level for subset of metrics in the leaf Prometheus. Such "federation" scrape reduce some unknowns when done across networks, because federate metric endpoint has all metric timestamps. Yet it usually suffers with inability to federate all metrics and inability to not lose data during longer network partitions (minutes).
* Remote Read allows to directly select metrics from the Prometheus database without PromQL engine. You can deploy Prometheus or other solution (e.g Thanos) on global level to perform PromQL query there while fetching required metrics from multiple remote locations. This is really powerful as it allows you to store data "locally" and access it only when needed. Unfortunately there are cons too. Without feature like [Query Pushdown](https://github.com/thanos-io/thanos/issues/305) we are in extreme cases pulling GBs of compressed metric data to answer a single query. Also, if we have a network partition we are temporarily blind. Last but not least, certain security guidelines are not allowing ingress traffic, only egress one.
* Finally, we have *Remote Write* which seems to be the most popular choice nowadays. Since the agent mode focuses on remote write feature use cases, let's explain it in more details.

The Prometheus Remote Write protocol allows us to forward (stream) all or subset of metrics collected by Prometheus to the remote location. You can configure Prometheus to forward some metrics (if you want, with metadata and exemplars!) to one or more locations that supports [Remote Write API](TODO)

TBD

If you would like hands-on experience of remote write capabilities, we recommend our [Thanos Katacoda tutorial of remote writing metrics from Prometheus](https://katacoda.com/thanos/courses/thanos/3-receiver) which explains all steps required for Prometheus to forward all metrics to remote location.

## Prometheus Agent Mode

TBD

## Summary

As always, if you have any issues or feedback, feel free to submit a ticket on GitHub or ask questions on the mailing list.
